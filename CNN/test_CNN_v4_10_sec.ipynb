{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ffec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Parameters for the model\n",
    "n_pixels = 28 * 28\n",
    "n_classes = 10\n",
    "\n",
    "# Parameters for the training\n",
    "USE_CPU = False\n",
    "reg_val = 1e-4\n",
    "lr = 0.001 / 2\n",
    "batchsize = 32\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts [0-255] RGBA to [0.0-1.0], shape [3, H, W]\n",
    "    transforms.Normalize(mean=[.5,.5,.5], std=[.5,.5,.5])  # Normalize for RGB\n",
    "])\n",
    "\n",
    "# --- 2. Load dataset ---\n",
    "dataset = datasets.ImageFolder(root='sliding_spectrograms_10_seconds', transform=transform)\n",
    "\n",
    "image, label = dataset[0]\n",
    "print(image.shape)  # The transformed image (e.g., torch.Size([3, H, W]))\n",
    "print(label)  # The class label (e.g., 0 for 'cat')\n",
    "\n",
    "\n",
    "# Split into train, val, test (80/10/10)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "trainset, valset, testset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "image, label = trainset[0] \n",
    "print(image.shape) # torch.Size([1, 28, 28])\n",
    "print(label) \n",
    "input_image_shape = image.shape\n",
    "\n",
    "print(f'Train set size: {len(trainset)}, Validation set size: {len(valset)}, Test set size: {len(testset)}')\n",
    "\n",
    "class FullDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset        \n",
    "        self.data, self.labels = self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(len(self.dataset)):\n",
    "            x, y = self.dataset[i]\n",
    "            data.append(x)\n",
    "            labels.append(y)\n",
    "        \n",
    "        return torch.stack(data), torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "trainset = FullDataset(trainset)\n",
    "valset = FullDataset(valset)\n",
    "\n",
    "# Shuffle the data at the start of each epoch (only useful for training set)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
    "train_eval_loader = DataLoader(trainset, batch_size=batchsize, shuffle=False)\n",
    "valloader = DataLoader(valset, batch_size=batchsize, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "# for an alternative approach, see: mini_vgg_like_torch_1.py\n",
    "def vgg_like_block(in_channels, num_filters, ksize=3, drate=0.25, pad='same'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, num_filters, kernel_size=ksize, padding=pad),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(num_filters),\n",
    "        nn.Conv2d(num_filters, num_filters, kernel_size=ksize, padding=pad),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(num_filters),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout2d(p=drate)\n",
    "    )\n",
    "\n",
    "def classifier_mlp(n_in, n_hidden, n_classes, drate=0.25):\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(n_in, n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.Dropout1d(p=drate),\n",
    "        nn.Linear(n_hidden, n_classes)\n",
    "    )\n",
    "\n",
    "class MiniVgg(nn.Module):\n",
    "    def __init__(self, b1_filters=32, b2_filters=64, H=480, W=640, fc_nodes=512, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.block1 = vgg_like_block(3, b1_filters)\n",
    "        self.block2 = vgg_like_block(b1_filters, b2_filters)\n",
    "        assert H % 4 == 0, f'the image height and width must be a multiple of 4: you passed H = {H}'\n",
    "        mlp_in_size = (H * W // 16) * b2_filters  # the H and W are both reduced by 4 with 2 max-pool layers.\n",
    "        self.classifier = classifier_mlp(mlp_in_size, fc_nodes, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        y = self.block1(x)\n",
    "        y = self.block2(y)\n",
    "        y = y.view(batch_size, -1)\n",
    "        y = self.classifier(y)\n",
    "        return y\n",
    "\n",
    "model = MiniVgg()\n",
    "print(model)\n",
    "summary(model, input_image_shape)  # call summary before moving the model to a device...\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # includes softmax (for numerical stability)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg_val)  # default learning rate is 0.001\n",
    "\n",
    "# set the device to use and move model to device\n",
    "\n",
    "if USE_CPU:\n",
    "    device = torch.device(\"cpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # MPS acceleration is available on MacOS 12.3+\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "model.to(device) # Move model to device\n",
    "\n",
    "\n",
    "# Define function to call for each training epoch (one complete pass over the training set)\n",
    "def train(model, trainloader, criterion, optimizer, device):\n",
    "    model.train() # set model to training mode\n",
    "    running_loss = 0; running_acc = 0\n",
    "    with tqdm(total=len(trainloader), desc=f\"Train\", unit=\"batch\") as pbar:\n",
    "        for n_batch, (images, labels) in enumerate(trainloader): # Iterate over batches\n",
    "            images, labels = images.to(device), labels.to(device) # Move batch to device\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images) # Forward pass\n",
    "            loss = criterion(output, labels) # Compute loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "            running_loss += loss.item()\n",
    "            running_acc += (output.argmax(1) == labels).float().mean().item()\n",
    "            # pbar.set_postfix({'loss': loss.item(), 'acc': 100. * running_acc / (n_batch+1)})\n",
    "            pbar.set_postfix({'loss': running_loss / (n_batch+1), 'acc': 100. * running_acc / (n_batch+1)})\n",
    "            pbar.update() # Update progress bar\n",
    "    return running_loss / len(trainloader), running_acc / len(trainloader) # return loss and accuracy for this epoch\n",
    "\n",
    "# Define function to call for each validation epoch (one complete pass over the validation set)\n",
    "def validate(model, valloader, criterion, device, tag='Val'):\n",
    "    model.eval() # set model to evaluation mode (e.g. turn off dropout, batchnorm, etc.)\n",
    "    running_loss = 0; running_acc = 0\n",
    "    with torch.no_grad(): # no need to compute gradients for validation\n",
    "        with tqdm(total=len(valloader), desc=tag, unit=\"batch\") as pbar:\n",
    "            for n_batch, (images, labels) in enumerate(valloader): # Iterate over batches\n",
    "                images, labels = images.to(device), labels.to(device) # Move batch to device\n",
    "                output = model(images) # Forward pass\n",
    "                loss = criterion(output, labels) # Compute loss\n",
    "                running_loss += loss.item() \n",
    "                running_acc += (output.argmax(1) == labels).float().mean().item()\n",
    "                pbar.set_postfix({'loss': running_loss / (n_batch+1), 'acc': 100. * running_acc / (n_batch+1)})\n",
    "                pbar.update() # Update progress bar\n",
    "    return running_loss / len(valloader), running_acc / len(valloader)  # return loss and accuracy for this epoch\n",
    "\n",
    "# Run training and validation loop\n",
    "# Save the best model based on validation accuracy\n",
    "n_epochs = 20\n",
    "best_acc = -1\n",
    "train_loss_history = []; train_acc_history = []\n",
    "val_loss_history = []; val_acc_history = []\n",
    "for epoch in range(n_epochs): # Iterate over epochs\n",
    "    print(f\"\\nEpoch {epoch+1} of {n_epochs}\")\n",
    "    if epoch == n_epochs // 2:\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Reducing learning rate from {lr} to {lr/4}')\n",
    "        optimizer.param_groups[0]['lr'] /= 4\n",
    "    train_loss, train_acc  = train(model, trainloader, criterion, optimizer, device) # Train\n",
    "    train_loss, train_acc  = validate(model, train_eval_loader, criterion, device, tag='Train Eval') # Evaluate on Train data\n",
    "    val_loss, val_acc = validate(model, valloader, criterion, device) # Validate\n",
    "    train_loss_history.append(train_loss); train_acc_history.append(train_acc)\n",
    "    val_loss_history.append(val_loss); val_acc_history.append(val_acc)\n",
    "    if val_acc > best_acc: # Save best model\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model_v3.pt\") # saving model parameters (\"state_dict\") saves memory and is faster than saving the entire model\n",
    "\n",
    "epochs = torch.arange(n_epochs)\n",
    "\n",
    "# plot training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss_history, label='train_loss')\n",
    "plt.plot(epochs, val_loss_history, label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Multiclass Cross Entropy Loss')\n",
    "plt.title(f'Loss with miniVGG model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc_history, label='train_acc')\n",
    "plt.plot(epochs, val_acc_history, label='val_acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with miniVGG model; Regularizer: {reg_val : 3.2g}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model and evaluate on test set\n",
    "model.load_state_dict(torch.load(\"best_model_v3.pt\"))\n",
    "# test_loss, test_acc = validate(model, testloader, criterion, device)\n",
    "# print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "model.eval() # set model to evaluation mode \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a96039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    " \n",
    "# model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "genre_list = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "\n",
    "for inputs, labels in testloader:  # Replace `testloader` with your DataLoader\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move to the appropriate device (GPU or CPU)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get the predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # Store all predictions and labels\n",
    "        all_preds.extend(preds.cpu().numpy())  # Convert to numpy and add to list\n",
    "        all_labels.extend(labels.cpu().numpy())  # Convert to numpy and add to list\n",
    "\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "class_labels = [genre_list[label] for label in all_labels]\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genre_list, yticklabels=genre_list)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
